{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRTRSiPb467j"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from pandas.tools.plotting import scatter_matrix\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV,RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.linear_model import Ridge,Lasso,BayesianRidge,SGDRegressor,ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
        "from sklearn.feature_selection import RFECV\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from scipy.stats import randint\n",
        "\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSkUG2CA5IrL"
      },
      "source": [
        "Mount your Google Drive. In this notebook, we assume that 'report1' folder is placed directly under 'My Drive'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56CkXhQu5Pe4"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD2mdnv55iw4"
      },
      "outputs": [],
      "source": [
        "# !ls /content/drive/My\\ Drive  #You should be able to see 'report1' folder by running this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSZdc1Aj467p"
      },
      "outputs": [],
      "source": [
        "root_dir=\"./\"\n",
        "N = 20000\n",
        "\n",
        "train_data = pd.read_csv(root_dir+\"train.csv\")\n",
        "test_data = pd.read_csv(root_dir+\"test.csv\")\n",
        "\n",
        "train_data = train_data.drop(['url'], axis=1) #remove 'url' information.\n",
        "train_data = train_data.drop(['timedelta'], axis=1) #remove 'url' information.\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] =6.0\n",
        "fig_size[1] = 6.0\n",
        "#plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "\n",
        "x = train_data['shares']\n",
        "plt.hist(x, normed=True, bins=250)\n",
        "plt.ylabel('shares');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u = np.median(train_data['shares'])\n",
        "s = np.std(train_data['shares'])\n",
        "line=u+2*s\n",
        "new_train_data=train_data[train_data['shares']<line]\n",
        "new_train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] =6.0\n",
        "fig_size[1] = 6.0\n",
        "#plt.rcParams[\"figure.figsize\"] = fig_size\n",
        "\n",
        "x = new_train_data['shares']\n",
        "plt.hist(x, normed=True, bins=250)\n",
        "plt.ylabel('shares');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Standarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# minmax_scaler = MinMaxScaler()\n",
        "# minmax_scaler.fit(new_train_data)\n",
        "# new_train_data = minmax_scaler.transform(new_train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_val= train_test_split(new_train_data, test_size=0.2, random_state=42)\n",
        "\n",
        "# y_train = np.matrix(X_train['shares']).T\n",
        "# X_train = np.matrix(X_train.drop(['shares'], axis=1)) #Dropping both 'shares', the predicted variable and 'url', a text variable\n",
        "\n",
        "# y_val = np.matrix(X_val['shares']).T\n",
        "# X_val = np.matrix(X_val.drop(['shares'], axis=1))\n",
        "\n",
        "# from sklearn import svm\n",
        "# from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "\n",
        "# svrgs_parameters = {\n",
        "#     'kernel': ['rbf', 'poly'],\n",
        "#     'C':      [1.0, 2.0, 3.0, 6.0, 10],\n",
        "#     'gamma':  ['scale', 'auto'],\n",
        "# }\n",
        "\n",
        "# svr_cv = GridSearchCV(svm.SVR(), svrgs_parameters, cv=8, scoring= 'neg_mean_squared_error')\n",
        "# svr_cv.fit(X_train, y_train)\n",
        "# print(\"SVR GridSearch score: \"+ str(svr_cv.best_score_))\n",
        "# print(\"SVR GridSearch params: \" + str(svr_cv.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_svr_best = SVR(kernel='rbf', C=6, gamma='auto')\n",
        "# model_svr_best.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# final_prediction = model_svr_best.predict(X_val)\n",
        "# def MABS(vec1, vec2):\n",
        "#     return np.mean(np.abs(vec1 - vec2))\n",
        "\n",
        "# # reg = Lasso(alpha=10.0).fit(XTrain, yTrain)   #alpha: regularization strength\n",
        "# yHatTrain = model_svr_best.predict(X_train)\n",
        "# yHatVal = model_svr_best.predict(X_val)\n",
        "# print(\"Training error \", MABS(y_train, yHatTrain.T))\n",
        "# print(\"Validation error \", MABS(y_val, yHatVal.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x = pd.get_dummies(new_train_data.iloc[:,1:59])\n",
        "# # MEDVとの相関性\n",
        "# plt.figure(figsize=(10,7))\n",
        "# x.corr()['shares'].sort_values(ascending = False).plot(kind='bar')\n",
        "# plt.title(\"Correlations between MEDV and variables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "c = new_train_data.corr().abs()\n",
        "s = c.unstack()\n",
        "so = s.sort_values(kind=\"quicksort\")\n",
        "so = pd.DataFrame(data=so, columns=['Pearson Coeff'])\n",
        "so = so[(so['Pearson Coeff'] >= 0.8) & (so['Pearson Coeff'] < 1.0)]\n",
        "so.iloc[::2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.set(style=\"white\")\n",
        "f = plt.figure(figsize = (10,10))\n",
        "sns.heatmap(new_train_data.corr(),cmap=sns.diverging_palette(220, 10, as_cmap=True),vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},mask=np.triu(np.ones_like(new_train_data.corr(), dtype=np.bool)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "N=200\n",
        "\n",
        "# X = np.matrix(train_data.drop(['shares'], axis=1)) \n",
        "# y = np.matrix(train_data['shares']) #This is the target\n",
        "X = new_train_data\n",
        "train_df = X.sample(frac=0.99, random_state = None)\n",
        "yTrain = train_df['shares']\n",
        "XTrain = train_df.drop(['shares'], axis=1) #use the first N samples for training\n",
        "# print(XTrain.shape,yTrain.shape)\n",
        "\n",
        "possible_features = new_train_data.columns.copy().drop('shares')\n",
        "# print(possible_features)\n",
        "selector = SelectKBest(f_regression, len(possible_features))\n",
        "selector.fit(XTrain[possible_features], yTrain)\n",
        "scores = -np.log10(selector.pvalues_)\n",
        "indices = np.argsort(scores)[::-1]\n",
        "# print('Feature importances:')\n",
        "topfeatures=[]\n",
        "for i in range(20):\n",
        "    # print('%.2f %s' % (scores[indices[i]], possible_features[indices[i]]))\n",
        "    topfeatures.append(possible_features[indices[i]])\n",
        "print(topfeatures)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# corr = new_train_data.corr()\n",
        "# flag = np. full((corr.shape[0]),True, dtype=bool)\n",
        "# for i in range(corr. shape[0]):\n",
        "#     for j in range(i+1, corr. shape[0]):\n",
        "#         if corr.iloc[i,j] >= 0.6:\n",
        "#             if flag[j]:\n",
        "#                 flag[j] = False\n",
        "# select = new_train_data.columns [flag]. tolist()\n",
        "# new_train_data = new_train_data[select]\n",
        "# select.remove( 'shares')\n",
        "# test_data = test_data[select]\n",
        "# print(new_train_data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_columns=['self_reference_avg_sharess','self_reference_max_shares','self_reference_min_shares',\n",
        "'global_sentiment_polarity','global_subjectivity',\n",
        "'kw_max_avg','kw_min_avg','kw_avg_max','kw_avg_avg',\n",
        "'num_hrefs', 'num_imgs','num_videos','num_keywords',\n",
        "'LDA_00','LDA_01','LDA_02','LDA_03',\n",
        "'weekday_is_saturday','weekday_is_sunday', 'is_weekend',\n",
        "'avg_negative_polarity',\n",
        "'rate_negative_words',\n",
        "'data_channel_is_entertainment', 'data_channel_is_tech','data_channel_is_world','data_channel_is_socmed']\n",
        "# selected_columns=topfeatures\n",
        "# selected_columns.append('shares')\n",
        "# new_train_data=new_train_data[selected_columns]\n",
        "# selected_columns\n",
        "# len(selected_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Features recomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "date_features=['weekday_is_saturday','weekday_is_sunday', 'is_weekend']\n",
        "# ref_features=['self_reference_avg_sharess','self_reference_max_shares','self_reference_min_shares']\n",
        "# topic2_features=['LDA_02','data_channel_is_world']\n",
        "# senti_features=['global_sentiment_polarity','global_subjectivity']\n",
        "dele_features=[date_features]#date_features,ref_features,topic2_features,senti_features\n",
        "recomp_features=['date']#'date','ref','topic2','senti'\n",
        "\n",
        "for dele_ind in range(len(dele_features)):\n",
        "    recomp_feature=recomp_features[dele_ind]\n",
        "    new_train_data[recomp_feature]=0\n",
        "    test_data[recomp_feature]=0\n",
        "    for dele_feature in dele_features[dele_ind]:\n",
        "        new_train_data[recomp_feature]+=new_train_data[dele_feature]\n",
        "        test_data[recomp_feature]+=test_data[dele_feature]\n",
        "        selected_columns.remove(dele_feature)\n",
        "    mask=(new_train_data[recomp_feature]>0)\n",
        "    new_train_data[recomp_feature][mask]=1\n",
        "    new_train_data[recomp_feature][~mask]=0\n",
        "    mask=(test_data[recomp_feature]>0)\n",
        "    test_data[recomp_feature][mask]=1\n",
        "    test_data[recomp_feature][~mask]=0\n",
        "    selected_columns.append(recomp_feature)\n",
        "\n",
        "test_data=test_data[selected_columns]\n",
        "selected_columns\n",
        "selected_columns.append('shares')\n",
        "new_train_data=new_train_data[selected_columns]\n",
        "selected_columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "target = new_train_data['shares']\n",
        "new_train_data=new_train_data.drop(['shares'], axis=1)\n",
        "\n",
        "minmax_scaler = StandardScaler()\n",
        "minmax_scaler.fit(new_train_data)\n",
        "new_train_data = minmax_scaler.transform(new_train_data)\n",
        "\n",
        "XTrain, XVal, yTrain, yVal = train_test_split(new_train_data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "yTrain = np.float32(yTrain).T\n",
        "XTrain = np.float32(XTrain) #Dropping both 'shares', the predicted variable and 'url', a text variable\n",
        "\n",
        "yVal = np.float32(yVal).T\n",
        "XVal = np.float32(XVal)\n",
        "\n",
        "minmax_scaler.fit(test_data)\n",
        "test_data = minmax_scaler.transform(test_data)\n",
        "XTest = np.float32(test_data) #final testing data\n",
        "print(XTrain.shape,yTrain.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# pipe_svr = Pipeline([(\"StandardScaler\",StandardScaler()),\n",
        "#                      (\"svr\",SVR())])\n",
        "# param_range = [1000.0,2000.0,5000.0,10000.0]\n",
        "# param_grid = [{\"svr__C\":param_range,\"svr__kernel\":[\"linear\"]}]\n",
        "#               # 注意__是指两个下划线，一个下划线会报错的\n",
        "#             #   {\"svr__C\":param_range,\"svr__gamma\":param_range,\"svr__kernel\":[\"rbf\"]}]\n",
        "# gs = GridSearchCV(estimator=pipe_svr,\n",
        "#                   param_grid = param_grid,\n",
        "#                   scoring = 'r2',\n",
        "#                   cv = 10)       # 10折交叉验证\n",
        "# gs = gs.fit(XTrain,yTrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"网格搜索最优得分：\",gs.best_score_)\n",
        "# print(\"网格搜索最优参数组合：\\n\",gs.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# svrgs_parameters = {\n",
        "#     # 'kernel': ['rbf', 'poly'],\n",
        "#     'C':      [1.0, 10.0, 100.0, 1000.0],\n",
        "#     # 'gamma':  ['scale', 'auto'],\n",
        "# }\n",
        "\n",
        "# svr_cv = GridSearchCV(SVR(), svrgs_parameters, cv=8, scoring= 'neg_mean_squared_error')\n",
        "# svr_cv.fit(XTrain, yTrain)\n",
        "# print(\"SVR GridSearch score: \"+ str(svr_cv.best_score_))\n",
        "# print(\"SVR GridSearch params: \" + str(svr_cv.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Cross_validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimator = SVR(kernel=\"linear\")\n",
        "# selector = RFECV(estimator, step=1, cv=3)\n",
        "# rfecv = selector.fit(XTrain, yTrain)\n",
        "\n",
        "# print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
        "# print('Selected features: %s' % list(X.columns[rfecv.support_]))\n",
        "\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# plt.xlabel(\"Number of features selected\")\n",
        "# plt.ylabel(\"Cross validation score (no. of correct classifications)\")\n",
        "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def MABS(vec1, vec2):\n",
        "    return np.mean(np.abs(vec1 - vec2))\n",
        "\n",
        "model=SVR(C=310)\n",
        "model.fit(XTrain, yTrain)\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yHatTest = model.predict(XTest)\n",
        "np.savetxt(root_dir+'result.txt', yHatTest) #save predictions in rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(XTrain, yTrain)\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(XTrain, yTrain)\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* GradientRegression 2151"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = GradientBoostingRegressor(random_state=42)\n",
        "model.fit(XTrain, yTrain)\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Bayes 2142"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BayesianRidge()\n",
        "model.fit(XTrain, yTrain)\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* SVR Feature selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVR_sel = SVR(kernel=\"linear\")  //linear效果不行不如rbf，但是rbf不支持特征挑选\n",
        "# selector=RFECV(SVR_sel,step=1,cv=5)\n",
        "# selector.fit(XTrain, yTrain)\n",
        "# selector.ranking_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# XTrain=XTrain[selector.support_]\n",
        "# yTrain=yTrain[selector.support_]\n",
        "# XTrain=XTest[selector.support_]\n",
        "# SVR_reg = SVR()\n",
        "# SVR_reg.fit(XTrain, yTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* SVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SVR_reg = SVR()\n",
        "# SVR_reg.fit(XTrain, yTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m7dAWcr54aO"
      },
      "source": [
        "Run Ridge regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMNkYM1p467v"
      },
      "outputs": [],
      "source": [
        "model = Lasso(alpha=10.0).fit(XTrain, yTrain)   #alpha: regularization strength\n",
        "yHatTrain = model.predict(XTrain)\n",
        "yHatVal = model.predict(XVal)\n",
        "print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "print(\"Validation error \", MABS(yVal, yHatVal.T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJYNZ-u7595p"
      },
      "source": [
        "Evaluate training and validation errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create dataset for lightgbm\n",
        "# lgb_train = lgb.Dataset(XTrain, yTrain)\n",
        "# lgb_eval = lgb.Dataset(XVal, yVal, reference=lgb_train)\n",
        "\n",
        "# specify your configurations as a dict\n",
        "# params = {\n",
        "# 'boosting_type': 'gbdt',\n",
        "# 'objective':'mae',\n",
        "# 'n_jobs':8,\n",
        "# 'subsample': 0.5,\n",
        "# 'subsample_freq': 1,\n",
        "# 'learning_rate': 0.01,\n",
        "# 'num_leaves': 2**11-1,\n",
        "# 'min_data_in_leaf': 2**12-1,\n",
        "# 'feature_fraction': 0.5,\n",
        "# 'max_bin': 100,\n",
        "# 'n_estimators': 2500,\n",
        "# 'boost_from_average': False,\n",
        "# \"random_seed\":1,\n",
        "# }\n",
        "\n",
        "# print('Starting training...')\n",
        "# # train\n",
        "# verbose=100\n",
        "# model = LGBMRegressor(**params)\n",
        "# model.fit(XTrain,yTrain,\n",
        "#                 eval_set=[(XVal,yVal)],\n",
        "#                 early_stopping_rounds=verbose,\n",
        "#                 verbose=verbose)\n",
        "# # with open(f'model_{num}.pkl', 'wb') as handle:\n",
        "# #     pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# def MABS(vec1, vec2):\n",
        "#     return np.mean(np.abs(vec1 - vec2))\n",
        "\n",
        "# yHatTrain = model.predict(XTrain)\n",
        "# yHatVal = model.predict(XVal)\n",
        "# print(\"Training error \", MABS(yTrain, yHatTrain.T))\n",
        "# print(\"Validation error \", MABS(yVal, yHatVal.T))\n",
        "\n",
        "# yHatTest=model.predict(XTest)\n",
        "# np.savetxt(root_dir+'result.txt', yHatTest) #save predictions in rows\n",
        "\n",
        "# params,\n",
        "#                 lgb_train,\n",
        "#                 num_boost_round=20,\n",
        "#                 valid_sets=lgb_eval,\n",
        "#                 early_stopping_rounds=5)\n",
        "\n",
        "# print('Saving model...')\n",
        "# # save model to file\n",
        "# gbm.save_model('model.txt')\n",
        "\n",
        "# print('Starting predicting...')\n",
        "# # predict\n",
        "# y_pred = gbm.predict(XTest, num_iteration=gbm.best_iteration)\n",
        "# eval\n",
        "# print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xum8f8Nu467y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eJ2XsU56CwN"
      },
      "source": [
        "Now, project the testing data. You can upload 'result.txt' to the evaluation server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOcWlWAn4672"
      },
      "outputs": [],
      "source": [
        "yHatTest = model.predict(XTest)\n",
        "np.savetxt(root_dir+'result.txt', yHatTest) #save predictions in rows"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "sample_sklearn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
